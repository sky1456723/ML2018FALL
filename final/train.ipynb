{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.utils\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEVICE ###\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "root_dir = os.path.join(\"../\",\"data\",\"ntu_final_data\")\n",
    "\n",
    "train_file = pd.read_csv(os.path.join(root_dir,\"medical_images\",\"train.csv\"))\n",
    "label_data = []\n",
    "unlabel_data = []\n",
    "for i in train_file.index:\n",
    "    if type(train_file.loc[i][\"Labels\"]) != str:\n",
    "        if math.isnan(train_file.loc[i][\"Labels\"]):\n",
    "            pass\n",
    "            '''\n",
    "            unlabel_data.append( [ train_file.loc[i][\"Image Index\"] ])\n",
    "            '''\n",
    "    else:\n",
    "        p = [train_file.loc[i][\"Image Index\"], train_file.loc[i][\"Labels\"]]\n",
    "        label_data.append(p)\n",
    "        \n",
    "        \n",
    "img_dirs = os.path.join(root_dir,\"medical_images\",\"images\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "transformList = []\n",
    "transformList.append(transforms.RandomResizedCrop(224))\n",
    "transformList.append(transforms.RandomHorizontalFlip())\n",
    "#transformList.append(transforms.ToTensor())\n",
    "#transformList.append(normalize)      \n",
    "transformSequence=transforms.Compose(transformList)\n",
    "def get_dataloader(data_list, transform=None, normalize=None):\n",
    "    part_data = []\n",
    "    part_label = []\n",
    "    print(len(data_list))\n",
    "    for i, pair in enumerate(data_list):\n",
    "        print(i,end='\\r')\n",
    "        img = Image.open(os.path.join(img_dirs, pair[0]))\n",
    "        if transform != None:\n",
    "            img = img.convert(mode=\"RGB\")\n",
    "            img = transform(img)\n",
    "            img = np.array(img)/255\n",
    "            img = np.transpose(img, axes=[2,0,1])\n",
    "        else:\n",
    "            img = img.convert(mode=\"RGB\")\n",
    "            img = img.resize((224,224))\n",
    "            img = np.array(img) / 255\n",
    "            img = np.transpose(img, axes=[2,0,1])\n",
    "        label = pair[1].split()\n",
    "        label = np.array([int(c) for c in label])\n",
    "        part_label.append(label)\n",
    "        part_data.append(img)\n",
    "\n",
    "    batch = 16\n",
    "    label_data_x = torch.Tensor(part_data)\n",
    "    label_data_y = torch.Tensor(part_label)\n",
    "    label_dataset = torch.utils.data.TensorDataset(label_data_x, label_data_y)\n",
    "    label_dataloader = torch.utils.data.DataLoader(dataset = label_dataset,\n",
    "                                                   batch_size =batch,\n",
    "                                                   shuffle = False,\n",
    "                                                   num_workers = 1 )\n",
    "    del part_data, part_label\n",
    "    return label_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    }
   ],
   "source": [
    "### Define Model ###\n",
    "base_model = torchvision.models.densenet121(pretrained = True)\n",
    "base_model.classifier = torch.nn.Linear(in_features = base_model.classifier.in_features,\n",
    "                                        out_features = 14,\n",
    "                                        bias = True)\n",
    "'''\n",
    "base_model.add_module(\"pre_conv\",\n",
    "                      torch.nn.Sequential(\n",
    "                          torch.nn.Conv2d(1,64, kernel_size=(8,8), stride = (2,2), padding = 1 ), #254x254\n",
    "                          torch.nn.BatchNorm2d(64),\n",
    "                          torch.nn.LeakyReLU(),\n",
    "                          torch.nn.Conv2d(64, 128, kernel_size=(5,5), stride=(1,1)),   #250x250\n",
    "                          torch.nn.BatchNorm2d(128),\n",
    "                          torch.nn.LeakyReLU(),\n",
    "                          torch.nn.Conv2d(128, 128, kernel_size=(6,6), stride=(1,1), dilation=2),   #240x240\n",
    "                          torch.nn.BatchNorm2d(128),\n",
    "                          torch.nn.LeakyReLU(),\n",
    "                          torch.nn.Conv2d(128, 128, kernel_size=(6,6), stride=(1,1)),   #230x230\n",
    "                          torch.nn.BatchNorm2d(128),\n",
    "                          torch.nn.LeakyReLU(),\n",
    "                          torch.nn.Conv2d(128, 128, kernel_size=(4,4), stride=(1,1), dilation=2),   #224x224\n",
    "                          torch.nn.BatchNorm2d(128),\n",
    "                          torch.nn.LeakyReLU(),\n",
    "                          torch.nn.Conv2d(128, 3, kernel_size=(1,1), stride=(1,1)),   #224x224\n",
    "                          torch.nn.BatchNorm2d(3),\n",
    "                          torch.nn.LeakyReLU(),\n",
    "                      )\n",
    "                     )\n",
    "'''\n",
    "base_model.add_module(\"output_act\",torch.nn.Sigmoid())\n",
    "base_model = base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Part  0\n",
      "900\n",
      "Part  1 56\n",
      "900\n",
      "Part  2 56\n",
      "900\n",
      "Part  3 56\n",
      "900\n",
      "Part  4 56\n",
      "901\n",
      "Part  5 56\n",
      "900\n",
      "Part  6 56\n",
      "900\n",
      "Part  7 56\n",
      "900\n",
      "Part  8 56\n",
      "900\n",
      "Part  9 56\n",
      "901\n",
      "Batch:  56\n",
      "Start Validation\n",
      "1000\n",
      "Batch:  62\n",
      "Epoch loss:  0.0949936746498066\n",
      "Epoch acc:  0.9509102163251691\n",
      "AUROC:  0.5194278194130922\n",
      "Epoch  1\n",
      "Part  0\n",
      "900\n",
      "Part  1 56\n",
      "900\n",
      "Part  2 56\n",
      "900\n",
      "Part  3 56\n",
      "900\n",
      "Part  4 56\n",
      "901\n",
      "Part  5 56\n",
      "900\n",
      "Part  6 56\n",
      "900\n",
      "Part  7 56\n",
      "900\n",
      "Part  8 56\n",
      "900\n",
      "Part  9 56\n",
      "901\n",
      "Batch:  56\n",
      "Start Validation\n",
      "1000\n",
      "Batch:  62\n",
      "Epoch loss:  0.09335311822143744\n",
      "Epoch acc:  0.9515768089259913\n",
      "AUROC:  0.4760431134675942\n",
      "Epoch  2\n",
      "Part  0\n",
      "900\n",
      "Part  1 56\n",
      "900\n",
      "Part  2 56\n",
      "900\n",
      "Part  3 56\n",
      "900\n",
      "Part  4 56\n",
      "901\n",
      "Part  5 56\n",
      "900\n",
      "Part  6 56\n",
      "900\n",
      "Part  7 56\n",
      "900\n",
      "Part  9 56\n",
      "901\n",
      "Batch:  56\n",
      "Start Validation\n",
      "1000\n",
      "Batch:  62\n",
      "Epoch loss:  0.0923425733029955\n",
      "Epoch acc:  0.9516720364403947\n",
      "AUROC:  0.5249736456568297\n",
      "Epoch  3\n",
      "Part  0\n",
      "900\n",
      "Part  1 56\n",
      "900\n",
      "Part  2 56\n",
      "900\n",
      "Part  3 56\n",
      "900\n",
      "Part  4 56\n",
      "901\n",
      "Part  5 56\n",
      "900\n",
      "Part  6 56\n",
      "900\n",
      "Part  7 56\n",
      "900\n",
      "Part  8 56\n",
      "900\n",
      "Part  9 56\n",
      "901\n",
      "Batch:  56\n",
      "Start Validation\n",
      "1000\n",
      "Batch:  62\n",
      "Epoch loss:  0.092079687970782\n",
      "Epoch acc:  0.9515926801783918\n",
      "AUROC:  0.4561874144550431\n",
      "Epoch  4\n",
      "Part  0\n",
      "900\n",
      "Part  1 56\n",
      "900\n",
      "Part  2 56\n",
      "900\n",
      "Part  3 56\n",
      "900\n",
      "Part  4 56\n",
      "901\n",
      "797\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  5 56\n",
      "900\n",
      "Part  6 56\n",
      "900\n",
      "Part  7 56\n",
      "900\n",
      "Part  8 56\n",
      "900\n",
      "Part  9 56\n",
      "901\n",
      "Batch:  56\n",
      "Start Validation\n",
      "1000\n",
      "Batch:  62\n",
      "Epoch loss:  0.09107315862805244\n",
      "Epoch acc:  0.9517355214499967\n",
      "AUROC:  0.5564194056660571\n",
      "Epoch  7\n",
      "Part  0\n",
      "900\n",
      "Part  1 56\n",
      "900\n",
      "Part  2 56\n",
      "900\n",
      "Part  3 56\n",
      "900\n",
      "Part  4 56\n",
      "901\n",
      "Part  5 56\n",
      "900\n",
      "Part  6 56\n",
      "900\n",
      "Part  7 56\n",
      "900\n",
      "Part  8 56\n",
      "900\n",
      "Part  9 56\n",
      "901\n",
      "Batch:  56\n",
      "Start Validation\n",
      "1000\n",
      "Batch:  34\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  3 56\n",
      "900\n",
      "Part  4 56\n",
      "901\n",
      "Part  5 56\n",
      "900\n",
      "Part  6 56\n",
      "900\n",
      "Part  7 56\n",
      "900\n",
      "Part  8 56\n",
      "900\n",
      "Part  9 56\n",
      "901\n",
      "Batch:  56\n",
      "Start Validation\n",
      "1000\n",
      "Batch:  621\n",
      "Epoch loss:  0.09011063784271489\n",
      "Epoch acc:  0.9517355214499967\n",
      "AUROC:  0.544435068928834\n",
      "Epoch  9\n",
      "Part  0\n",
      "900\n",
      "Part  1 56\n",
      "900\n",
      "Part  2 56\n",
      "900\n",
      "Part  3 56\n",
      "900\n",
      "Part  4 56\n",
      "901\n",
      "Part  5 56\n",
      "900\n",
      "Part  6 56\n",
      "900\n",
      "Part  7 56\n",
      "900\n",
      "Part  8 56\n",
      "900\n",
      "Part  9 56\n",
      "901\n",
      "Batch:  56\n",
      "Start Validation\n",
      "1000\n",
      "Batch:  62\n",
      "Epoch loss:  0.09008761276908483\n",
      "Epoch acc:  0.9517355214499967\n",
      "AUROC:  0.5532156063832149\n",
      "Epoch  10\n",
      "Part  0\n",
      "900\n",
      "Part  1 56\n",
      "900\n",
      "Part  2 56\n",
      "900\n",
      "Part  3 56\n",
      "900\n",
      "Part  4 56\n",
      "901\n",
      "Part  5 56\n",
      "900\n",
      "Part  6 56\n",
      "900\n",
      "Part  7 56\n",
      "900\n",
      "Part  8 56\n",
      "900\n",
      "Part  9 56\n",
      "901\n",
      "Batch:  56\n",
      "Start Validation\n",
      "1000\n",
      "Batch:  62\n",
      "Epoch loss:  0.08981564253280593\n",
      "Epoch acc:  0.9517355214499967\n",
      "AUROC:  0.5638710256478148\n",
      "Epoch  11\n",
      "Part  0\n",
      "900\n",
      "Part  1 56\n",
      "900\n",
      "Part  2 56\n",
      "900\n",
      "Part  3 56\n",
      "900\n",
      "Part  4 56\n",
      "901\n",
      "Part  5 56\n",
      "900\n",
      "Part  6 56\n",
      "900\n",
      "Part  7 56\n",
      "900\n",
      "Part  8 56\n",
      "900\n",
      "Part  9 56\n",
      "901\n",
      "Batch:  56\n",
      "Start Validation\n",
      "1000\n",
      "Batch:  62\n",
      "Epoch loss:  0.08939398365436084\n",
      "Epoch acc:  0.9517355214499967\n",
      "AUROC:  0.582651803106993\n",
      "Epoch  12\n",
      "Part  0\n",
      "900\n",
      "Part  1 56\n",
      "900\n",
      "Part  2 56\n",
      "900\n",
      "Part  3 56\n",
      "900\n",
      "Part  4 56\n",
      "901\n",
      "Part  5 56\n",
      "900\n",
      "Part  6 56\n",
      "900\n",
      "Part  7 56\n",
      "900\n",
      "Part  8 56\n",
      "900\n",
      "Part  9 56\n",
      "901\n",
      "Batch:  56\n",
      "Start Validation\n",
      "1000\n",
      "Batch:  62\n",
      "Epoch loss:  0.08956526024992394\n",
      "Epoch acc:  0.9517037789451956\n",
      "AUROC:  0.623756203267529\n",
      "Epoch  13\n",
      "Part  0\n",
      "900\n",
      "Part  1 56\n",
      "900\n",
      "Part  2 56\n",
      "900\n",
      "Part  3 56\n",
      "900\n",
      "Part  4 56\n",
      "901\n",
      "Part  5 56\n",
      "900\n",
      "Part  6 56\n",
      "900\n",
      "Part  7 56\n",
      "900\n",
      "Part  8 56\n",
      "900\n",
      "Part  9 56\n",
      "901\n",
      "Batch:  56\n",
      "Start Validation\n",
      "1000\n",
      "Batch:  62\n",
      "Epoch loss:  0.08916948343856959\n",
      "Epoch acc:  0.9517355214499967\n",
      "AUROC:  0.591932791233327\n",
      "Epoch  14\n",
      "Part  0\n",
      "900\n",
      "Part  1 56\n",
      "900\n",
      "Part  2 56\n",
      "900\n",
      "Part  3 56\n",
      "900\n",
      "Part  4 56\n",
      "901\n",
      "Part  5 56\n",
      "900\n",
      "Part  6 56\n",
      "900\n",
      "Part  7 56\n",
      "900\n",
      "Part  8 56\n",
      "900\n",
      "Part  9 56\n",
      "901\n",
      "Batch:  56\n",
      "Start Validation\n",
      "1000\n",
      "Batch:  62\n",
      "Epoch loss:  0.0886288612051257\n",
      "Epoch acc:  0.9517355214499967\n",
      "AUROC:  0.6037968572460546\n",
      "Epoch  15\n",
      "Part  0\n",
      "900\n",
      "Part  1 56\n",
      "900\n",
      "Part  2 56\n",
      "900\n",
      "Part  3 56\n",
      "900\n",
      "Part  4 56\n",
      "901\n",
      "Part  5 56\n",
      "900\n",
      "Part  6 56\n",
      "900\n",
      "Part  7 56\n",
      "900\n",
      "Part  8 56\n",
      "900\n",
      "Part  9 56\n",
      "901\n",
      "Batch:  56\n",
      "Start Validation\n",
      "1000\n",
      "Batch:  62\n",
      "Epoch loss:  0.08935862863054542\n",
      "Epoch acc:  0.9517355214499967\n",
      "AUROC:  0.600119064092795\n",
      "Epoch  16\n",
      "Part  0\n",
      "900\n",
      "Part  1 56\n",
      "900\n",
      "Part  2 56\n",
      "900\n",
      "Part  3 56\n",
      "900\n",
      "Part  4 56\n",
      "901\n",
      "Part  5 56\n",
      "900\n",
      "Part  6 56\n",
      "900\n",
      "Part  7 56\n",
      "900\n",
      "Part  8 56\n",
      "900\n",
      "Part  9 56\n",
      "901\n",
      "Batch:  56\n",
      "Start Validation\n",
      "1000\n",
      "Batch:  62\n",
      "Epoch loss:  0.08877771967001166\n",
      "Epoch acc:  0.9517355214499967\n",
      "AUROC:  0.5794302175111561\n",
      "Epoch  17\n",
      "Part  0\n",
      "900\n",
      "Part  1 56\n",
      "900\n",
      "Part  2 56\n",
      "900\n",
      "Part  3 56\n",
      "900\n",
      "Part  4 56\n",
      "901\n",
      "Part  5 56\n",
      "900\n",
      "Part  6 56\n",
      "900\n",
      "Part  7 56\n",
      "900\n",
      "Batch:  56\n",
      "Start Validation\n",
      "1000\n",
      "Batch:  62\n",
      "Epoch loss:  0.08909822293035641\n",
      "Epoch acc:  0.9517355214499967\n",
      "AUROC:  0.593679716275025\n",
      "Epoch  18\n",
      "Part  0\n",
      "900\n",
      "Part  1 56\n",
      "900\n",
      "Part  2 56\n",
      "900\n",
      "Part  3 56\n",
      "900\n",
      "Part  4 56\n",
      "901\n",
      "Part  5 56\n",
      "900\n",
      "Part  6 56\n",
      "900\n",
      "Part  7 56\n",
      "900\n",
      "Part  8 56\n",
      "900\n",
      "Part  9 56\n",
      "901\n",
      "Batch:  56\n",
      "Start Validation\n",
      "1000\n",
      "Batch:  62\n",
      "Epoch loss:  0.08853456262098049\n",
      "Epoch acc:  0.9517355214499967\n",
      "AUROC:  0.5982354710053769\n",
      "Epoch  19\n",
      "Part  0\n",
      "900\n",
      "Part  1 56\n",
      "900\n",
      "Part  2 56\n",
      "900\n",
      "Part  3 56\n",
      "900\n",
      "Part  4 56\n",
      "901\n",
      "740\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part  5 56\n",
      "900\n",
      "Part  6 56\n",
      "900\n",
      "Part  7 56\n",
      "900\n",
      "Batch:  54\r"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(base_model.parameters(),lr=0.001)\n",
    "#preconv_optimizer = torch.optim.Adam(base_model.pre_conv.parameters(),lr=0.001)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "epoch = 200\n",
    "model_name = \"224_2.pkl\"\n",
    "\n",
    "train_data = label_data[len(label_data)//10:]\n",
    "val_data = label_data[:len(label_data)//10]\n",
    "\n",
    "\n",
    "for e in range(epoch):\n",
    "    print(\"Epoch \",e)\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for part in range(10):\n",
    "        gc.collect()\n",
    "        print(\"Part \",part)\n",
    "        label_dataloader = get_dataloader(train_data[part*len(train_data)//10:(part+1)*len(train_data)//10],\n",
    "                                         transform = transformSequence,\n",
    "                                         normalize = None)\n",
    "        for b_num, (data, label) in enumerate(label_dataloader):\n",
    "            print(\"Batch: \", b_num, end='\\r')\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            #preconv_optimizer.zero_grad()\n",
    "            pred = base_model.output_act( base_model(data) )\n",
    "            loss = criterion(pred,label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #preconv_optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += torch.sum(torch.eq((pred>0.5), label.byte())).item()/14\n",
    "        del label_dataloader\n",
    "        torch.save(base_model, model_name)\n",
    "    print(\"\")\n",
    "    print(\"Start Validation\")\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    val_dataloader = get_dataloader(val_data)\n",
    "    ans_list = []\n",
    "    label_list = []\n",
    "    for b_num, (data, label) in enumerate(val_dataloader):\n",
    "        print(\"Batch: \", b_num, end='\\r')\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        pred = base_model.output_act( base_model(data) )\n",
    "        for one_row in pred.cpu().data.numpy():\n",
    "            ans_list.append(one_row)\n",
    "        for one_row in label.cpu().data.numpy():\n",
    "            label_list.append(one_row)\n",
    "    del val_dataloader\n",
    "    auroc = sklearn.metrics.roc_auc_score(np.array(label_list),np.array(ans_list))\n",
    "    print(\"\")\n",
    "    print(\"Epoch loss: \",8*epoch_loss/(9*len(label_data)//10) )\n",
    "    print(\"Epoch acc: \",epoch_acc/(9*len(label_data)//10) )\n",
    "    print(\"AUROC: \",auroc )\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
